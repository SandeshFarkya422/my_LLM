{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0604e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b43b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cpu\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device in use: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f85fe367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.rev_vocab = {}\n",
    "        self.token_count = 0\n",
    "\n",
    "    def bulid_vocab(self, texts,min_freq=2):\n",
    "        all_chars=[]\n",
    "        for text in texts:\n",
    "            cleaned_text =re.sub(r'[^a-zA-Z0-9\\s]', '', text)     \n",
    "            all_chars.extend(list(cleaned_text))\n",
    "\n",
    "        char_counts = Counter(all_chars)\n",
    "\n",
    "        # Add sppecial tokens\n",
    "        self.vocab= {\n",
    "            '<pad>': 0,\n",
    "            '<unk>': 1,\n",
    "            '<sos>': 2,\n",
    "            '<eos>': 3\n",
    "        }\n",
    "        self.inx_to_token = {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>'}\n",
    "\n",
    "        indx=4\n",
    "        for char,count in char_counts.items():\n",
    "            if count >= min_freq:\n",
    "                self.vocab[char] = indx\n",
    "                self.inx_to_token[indx] = char\n",
    "                indx += 1    \n",
    "            \n",
    "            def encode(self,token):    \n",
    "                tokens =[self.vocab['<bos>']]\n",
    "                for char in text:\n",
    "                    tokens.append(self.vocab.get(char, self.vocab['<unk>']))\n",
    "                tokens.append(self.vocab['<eos>'])\n",
    "                return tokens\n",
    "            \n",
    "            def decode(self,token_ids):\n",
    "                text = \"\"\n",
    "                for token_id in token_ids:\n",
    "                    if token_id in [self.vocab['<pad>'],self.vocab['<bps>'],self.vocab['<eos>']]:\n",
    "                        continue\n",
    "                    elif token_id == self.vocab['<unk>']:\n",
    "                        text += '?'\n",
    "                    else:\n",
    "                        text += self.inx_to_token(token_id, '')\n",
    "                return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "196a8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts_hi = [\n",
    "    \"नमस्ते, आप कैसे हैं?\",\n",
    "    \"यह एक साधारण उदाहरण है।\",\n",
    "    \"पायथन प्रोग्रामिंग भाषा बहुत लोकप्रिय है।\",\n",
    "    \"मुझे किताबें पढ़ना पसंद है।\",\n",
    "    \"आपका दिन शुभ हो!\",\n",
    "    \"क्या आप चाय पीना चाहेंगे?\",\n",
    "    \"हिंदी भाषा में संवाद करना अच्छा लगता है।\",\n",
    "    \"यहां कुछ और वाक्य हैं।\",\n",
    "    \"आइए हम एक साथ सीखते हैं।\",\n",
    "    \"आपका स्वागत है!\",\n",
    "    \"मौसम आज बहुत सुहाना है।\",\n",
    "    \"मैं हर दिन कुछ नया सीखता हूँ।\",\n",
    "    \"क्या आप मेरी मदद कर सकते हैं?\",\n",
    "    \"कृपया दरवाज़ा बंद कर दें।\",\n",
    "    \"मुझे संगीत सुनना अच्छा लगता है।\",\n",
    "    \"हम कल बाजार जाएंगे।\",\n",
    "    \"तुम क्या कर रहे हो?\",\n",
    "    \"यह खाना बहुत स्वादिष्ट है।\",\n",
    "    \"कृपया मुझे माफ़ कर दें।\",\n",
    "    \"आज स्कूल में छुट्टी है।\",\n",
    "    \"तुम्हारा नाम क्या है?\",\n",
    "    \"मुझे हिंदी में लिखना पसंद है।\",\n",
    "    \"मैं दिल्ली में रहता हूँ।\",\n",
    "    \"हम जल्दी मिलेंगे।\",\n",
    "    \"यह बहुत कठिन सवाल है।\",\n",
    "    \"यह समाधान सही लगता है।\",\n",
    "    \"कृपया धीरे बोलिए।\",\n",
    "    \"मैं थोड़ा व्यस्त हूँ।\",\n",
    "    \"आपका उत्तर सही है।\",\n",
    "    \"क्या आप मुझे सुन सकते हैं?\",\n",
    "    \"मैंने यह किताब पढ़ी है।\",\n",
    "    \"यह दृश्य बहुत सुंदर है।\",\n",
    "    \"आज बहुत गर्मी है।\",\n",
    "    \"मैं शाम को टहलने जाता हूँ।\",\n",
    "    \"क्या आप खेलना पसंद करते हैं?\",\n",
    "    \"मुझे आपकी बात पसंद आई।\",\n",
    "    \"आपका मोबाइल कहां है?\",\n",
    "    \"कृपया लाइट बंद कर दें।\",\n",
    "    \"मैं कल फिर आऊंगा।\",\n",
    "    \"हम साथ में फिल्म देखेंगे।\",\n",
    "    \"आपका पसंदीदा रंग कौन सा है?\",\n",
    "    \"मुझे नीला रंग पसंद है।\",\n",
    "    \"कृपया अपना नाम बताइए।\",\n",
    "    \"क्या आप तैयार हैं?\",\n",
    "    \"मैंने अपना होमवर्क पूरा कर लिया।\",\n",
    "    \"यह बहुत अच्छा विचार है।\",\n",
    "    \"आप बहुत अच्छे दोस्त हैं।\",\n",
    "    \"क्या आप खाना खा चुके हैं?\",\n",
    "    \"मैंने नाश्ता कर लिया है।\",\n",
    "    \"क्या हम अब शुरू करें?\",\n",
    "    \"मुझे आपके साथ काम करना अच्छा लगा।\",\n",
    "    \"हमने बहुत मज़ा किया।\",\n",
    "    \"क्या आप मुझे देख सकते हैं?\",\n",
    "    \"मैं अभ्यास कर रहा हूँ।\",\n",
    "    \"आज सोमवार है।\",\n",
    "    \"कल रविवार था।\",\n",
    "    \"मुझे गर्म दूध पसंद है।\",\n",
    "    \"कृपया दरवाज़ा खोलिए।\",\n",
    "    \"यह गीत बहुत मधुर है।\",\n",
    "    \"मैंने एक सुंदर सपना देखा।\",\n",
    "    \"हम पार्क में खेलते हैं।\",\n",
    "    \"तुम बहुत होशियार हो।\",\n",
    "    \"यह तो बहुत आसान है।\",\n",
    "    \"क्या तुम मुझे पहचानते हो?\",\n",
    "    \"मैं रोज़ व्यायाम करता हूँ।\",\n",
    "    \"मुझे फल खाना पसंद है।\",\n",
    "    \"मैंने अपना काम खत्म कर लिया।\",\n",
    "    \"कृपया थोड़ी देर रुकिए।\",\n",
    "    \"क्या यह आपकी किताब है?\",\n",
    "    \"मुझे ठंड लग रही है।\",\n",
    "    \"मैं कुछ कहना चाहता हूँ।\",\n",
    "    \"क्या आप अंग्रेज़ी बोलते हैं?\",\n",
    "    \"मैं थोड़ा बहुत जानता हूँ।\",\n",
    "    \"आपका दिन कैसा रहा?\",\n",
    "    \"मुझे थोड़ी मदद चाहिए।\",\n",
    "    \"यह मेरा पसंदीदा खेल है।\",\n",
    "    \"मैं आज खुश हूँ।\",\n",
    "    \"कृपया ध्यान दीजिए।\",\n",
    "    \"मुझे हिंदी में बोलना अच्छा लगता है।\",\n",
    "    \"यह समय बहुत महत्वपूर्ण है।\",\n",
    "    \"मैं नए शब्द सीख रहा हूँ।\",\n",
    "    \"आपके विचार प्रेरणादायक हैं।\",\n",
    "    \"मैंने अपनी योजना बदल दी है।\",\n",
    "    \"तुम बहुत अच्छे दिख रहे हो।\",\n",
    "    \"क्या हम मिल सकते हैं?\",\n",
    "    \"मुझे थोड़ा विश्राम चाहिए।\",\n",
    "    \"यह स्थान बहुत शांत है।\",\n",
    "    \"हम जल्दी निकलेंगे।\",\n",
    "    \"तुम कब वापस आओगे?\",\n",
    "    \"मैं कल यात्रा पर जा रहा हूँ।\",\n",
    "    \"आपका स्वास्थ्य कैसा है?\",\n",
    "    \"मुझे नींद आ रही है।\",\n",
    "    \"आपका पसंदीदा खाना क्या है?\",\n",
    "    \"मैंने अभी खाना खाया है।\",\n",
    "    \"आपकी मुस्कान बहुत प्यारी है।\",\n",
    "    \"मैंने यह पहले सुना है।\",\n",
    "    \"क्या आप दोहराएंगे?\",\n",
    "    \"मैं समझ नहीं पाया।\",\n",
    "    \"कृपया फिर से कहिए।\",\n",
    "    \"यह जानकारी उपयोगी है।\",\n",
    "    \"मुझे यात्रा करना पसंद है।\",\n",
    "    \"मैं अभी रास्ते में हूँ।\",\n",
    "    \"क्या आपको मेरी आवाज़ आ रही है?\",\n",
    "    \"मैं आपसे सहमत हूँ।\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9746d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5\n"
     ]
    }
   ],
   "source": [
    "tokenizer= SimpleTokenizer()\n",
    "tokenizer.bulid_vocab(sample_texts_hi)\n",
    "print(\"Vocabulary size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ce93cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self,texts,tokenizer,max_length=50):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        for text in texts:\n",
    "            tokens = tokenizer.encode(text)\n",
    "            if len(tokens) > max_length:\n",
    "                tokens = tokens[:max_length]\n",
    "            else:\n",
    "                tokens += [tokenizer.vocab['<pad>']] * (max_length - len(tokens))\n",
    "            self.data.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        input_ind= torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        target_ind = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return input_ind, target_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f49c7d24",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleTokenizer' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset\u001b[38;5;241m=\u001b[39m  \u001b[43mTestDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_texts_hi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)                                   \n",
      "Cell \u001b[1;32mIn[54], line 8\u001b[0m, in \u001b[0;36mTestDataset.__init__\u001b[1;34m(self, texts, tokenizer, max_length)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m----> 8\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(text)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m>\u001b[39m max_length:\n\u001b[0;32m     10\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m tokens[:max_length]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SimpleTokenizer' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "dataset=  TestDataset(sample_texts_hi, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ae69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
